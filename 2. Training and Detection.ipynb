{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QUANWN3rpfC9"
   },
   "source": [
    "# 0. Setup Paths\n",
    "This code block is dedicated to setting up the environment for our project. We're defining some important variables and also creating directories in our workspace. Here's a brief overview:\n",
    "\n",
    "1. **Model Selection:** We select the pre-trained model we want to fine-tune. In this case, we are using the SSD MobileNet V2 FPNLite 320x320 model, which is a fast and efficient object detection model. We store the model's name and download URL for later use.\n",
    "2. **File Names:** We set the names of some files that we will use later, such as the script to generate TensorFlow records and the label map file.\n",
    "3. **Paths:** We create a dictionary to store the paths to various directories we'll use throughout our project. This includes the workspace, scripts, models, annotations, images, and more. We'll use these paths to read and write data as we proceed.\n",
    "4. **File Paths:** Similarly, we create a dictionary for file paths that we will use throughout the project.\n",
    "5. **Directory Creation:** Finally, we iterate through all the path values in our paths dictionary. If any of these directories do not already exist, we create them using the mkdir command. This ensures we have a well-structured workspace before we begin our project.\n",
    "\n",
    "By organizing our workspace in this manner, we ensure that we have a neat and systematic setup, making it easy to manage our project as it becomes more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "146BB11JpfDA"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42hJEdo_pfDB"
   },
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' \n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "LABEL_MAP_NAME = 'label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbPhYVy_pfDB"
   },
   "outputs": [],
   "source": [
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('Tensorflow','protoc')\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwhWZMI0pfDC"
   },
   "outputs": [],
   "source": [
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "    'LABELMAP': os.path.join(paths['ANNOTATION_PATH'], LABEL_MAP_NAME)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HR-TfDGrpfDC"
   },
   "outputs": [],
   "source": [
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        if os.name == 'posix':\n",
    "            !mkdir -p {path}\n",
    "        if os.name == 'nt':\n",
    "            !mkdir {path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OLU-rs_ipfDE"
   },
   "source": [
    "# 1. Download TF Models Pretrained Models from Tensorflow Model Zoo and Install TFOD\n",
    "\n",
    "This code performs several operations needed for the setup of the TensorFlow Object Detection API. It starts by installing necessary dependencies based on the operating system type (Windows or Linux). Then, it clones the TensorFlow Object Detection API from the official repository if it's not already installed. For Linux, it installs protobuf-compiler and sets up the Object Detection API, while for Windows, it downloads protobuf, sets it up along with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/install/source_windows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depending on the operating system, the installation and setup process varies.\n",
    "# For a Windows system:\n",
    "if os.name=='nt':\n",
    "    # Install wget python package\n",
    "    !pip install wget\n",
    "    import wget\n",
    "\n",
    "# Checking if the TensorFlow Object Detection API is already installed. If not, clone it from the official GitHub repository.\n",
    "if not os.path.exists(os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection')):\n",
    "    !git clone https://github.com/tensorflow/models {paths['APIMODEL_PATH']}\n",
    "\n",
    "# If the operating system is Linux, install protobuf-compiler and setup the TensorFlow Object Detection API.\n",
    "if os.name=='posix':  \n",
    "    !apt-get install protobuf-compiler\n",
    "    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && cp object_detection/packages/tf2/setup.py . && python3 -m pip install .\n",
    "\n",
    "# If the operating system is Windows, download protobuf, setup it and the TensorFlow Object Detection API.\n",
    "if os.name=='nt':\n",
    "    url=\"https://github.com/protocolbuffers/protobuf/releases/download/v3.15.6/protoc-3.15.6-win64.zip\"\n",
    "    wget.download(url)\n",
    "    !move protoc-3.15.6-win64.zip {paths['PROTOC_PATH']}\n",
    "    !cd {paths['PROTOC_PATH']} && tar -xf protoc-3.15.6-win64.zip\n",
    "    os.environ['PATH'] += os.pathsep + os.path.abspath(os.path.join(paths['PROTOC_PATH'], 'bin'))   \n",
    "    !cd Tensorflow/models/research && protoc object_detection/protos/*.proto --python_out=. && copy object_detection\\\\packages\\\\tf2\\\\setup.py setup.py && python setup.py build && python setup.py install\n",
    "    !cd Tensorflow/models/research/slim && pip install -e .\n",
    "\n",
    "# Run a script provided by TensorFlow to verify the installation.\n",
    "VERIFICATION_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'builders', 'model_builder_tf2_test.py')\n",
    "!python {VERIFICATION_SCRIPT}\n",
    "\n",
    "# Upgrade tensorflow\n",
    "!pip install tensorflow --upgrade\n",
    "\n",
    "# Uninstall and re-install protobuf and matplotlib due to version conflicts\n",
    "!pip uninstall protobuf matplotlib -y\n",
    "!pip install protobuf matplotlib==3.2\n",
    "\n",
    "# Import the object detection module\n",
    "import object_detection\n",
    "\n",
    "# Print a list of installed python packages for verification\n",
    "!pip list\n",
    "\n",
    "# Download and extract the pre-trained model from TensorFlow model zoo based on the type of operating system.\n",
    "if os.name =='posix':\n",
    "    !wget {PRETRAINED_MODEL_URL}\n",
    "    !mv {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}\n",
    "if os.name == 'nt':\n",
    "    wget.download(PRETRAINED_MODEL_URL)\n",
    "    !move {PRETRAINED_MODEL_NAME+'.tar.gz'} {paths['PRETRAINED_MODEL_PATH']}\n",
    "    !cd {paths['PRETRAINED_MODEL_PATH']} && tar -zxvf {PRETRAINED_MODEL_NAME+'.tar.gz'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "M5KJTnkfpfDC"
   },
   "source": [
    "# 2. Create Label Map\n",
    "This section of code is creating a label map for the custom object detection model. The label map tells the model what each object is by assigning an ID to a specific object. In this case, we are telling the model that the object with ID 1 is a 'Nut' and the object with ID 2 is a 'Screw'.\n",
    "\n",
    "The label map is saved in the 'protobuf' (.pbtxt) format, which is the required format for TensorFlow's Object Detection API. Each item in the label map is defined by its name and ID. This label map file will later be used in the training and detection stages of the object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1BVDWo7pfDC"
   },
   "outputs": [],
   "source": [
    "labels = [{'name':'Nut', 'id':1}, {'name':'Screw', 'id':2}]\n",
    "\n",
    "with open(files['LABELMAP'], 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "C88zyVELpfDC"
   },
   "source": [
    "# 3. Create TF records\n",
    "First, it checks if there is a tarball archive (.tar.gz) of images present in the defined IMAGE_PATH. If such an archive exists, it uncompresses the archive using the tar command. The -zxvf flag tells tar to extract the files (-x), be verbose and print the file names as they are extracted (-v), use GZIP to uncompress (-z), and read from the specified file (-f).\n",
    "\n",
    "Then, it clones a Git repository that contains a script to generate TensorFlow records (TFRecords). TFRecords are a binary file format for storing data, and TensorFlow's preferred format for storing large amounts of data like images. The repository is cloned into the previously defined SCRIPTS_PATH.\n",
    "\n",
    "Finally, it runs the generate_tfrecord.py script. This script converts the image and corresponding annotation data into the TFRecord format. The script takes a few arguments:\n",
    "\n",
    "- -x or --xml_dir: The path to the directory containing the xml files with the bounding box annotations. In this case, it is the train directory inside IMAGE_PATH.\n",
    "- -l or --labels_path: The path to the label_map.pbtxt file, which was generated in a previous step.\n",
    "- -o or --output_path: The path where the generated TFRecord file will be saved. Here, it's the train.record file inside ANNOTATION_PATH.\n",
    "After this script runs, the training data will be in the format expected by TensorFlow's Object Detection API and ready to use for training the object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvf5WccwrFGq",
    "outputId": "49902aeb-0bd7-4298-e1a0-5b4a64eb2064"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL IF RUNNING ON COLAB\n",
    "ARCHIVE_FILES = os.path.join(paths['IMAGE_PATH'], 'archive.tar.gz')\n",
    "if os.path.exists(ARCHIVE_FILES):\n",
    "  !tar -zxvf {ARCHIVE_FILES}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWpb_BVUpfDD",
    "outputId": "56ce2a3f-3933-4ee6-8a9d-d5ec65f7d73c"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(files['TF_RECORD_SCRIPT']):\n",
    "    !git clone https://github.com/nicknochnack/GenerateTFRecord {paths['SCRIPTS_PATH']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPFToGZqpfDD",
    "outputId": "0ebb456f-aadc-4a1f-96e6-fbfec1923e1c"
   },
   "outputs": [],
   "source": [
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'train')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'train.record')} \n",
    "!python {files['TF_RECORD_SCRIPT']} -x {os.path.join(paths['IMAGE_PATH'], 'test')} -l {files['LABELMAP']} -o {os.path.join(paths['ANNOTATION_PATH'], 'test.record')} "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qT4QU7pLpfDE"
   },
   "source": [
    "# 4. Copy Model Config to Training Folder\n",
    "This copies the pipeline.config file from the pre-trained model's directory to your model's training directory.\n",
    "\n",
    "In the context of TensorFlow's Object Detection API, the pipeline.config file is a configuration file that defines various parameters for the object detection task such as the model architecture, the path for the train and validation data, the path for the label map, the number of classes, the batch size, the number of steps, learning rate, etc.\n",
    "\n",
    "This configuration file serves as the recipe for training a specific type of model. The original pipeline.config file comes from the pretrained model's directory, which means it was configured for that specific model. By copying it into your training directory, you ensure that your model will be trained with the same configuration. However, you may need to update some paths and parameters in the configuration file to fit your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOjuTFbwpfDF"
   },
   "outputs": [],
   "source": [
    "if os.name =='posix':\n",
    "    !cp {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}\n",
    "if os.name == 'nt':\n",
    "    !copy {os.path.join(paths['PRETRAINED_MODEL_PATH'], PRETRAINED_MODEL_NAME, 'pipeline.config')} {os.path.join(paths['CHECKPOINT_PATH'])}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga8gpNslpfDF"
   },
   "source": [
    "# 5. Update Config For Transfer Learning\n",
    "This section of the code is related to loading and modifying the pipeline.config file for training your own model. The pipeline.config file is read, updated and then written back to the disk.\n",
    "\n",
    "Here is the detailed explanation of each part:\n",
    "\n",
    "1. TensorFlow and various utilities from the Object Detection API are imported, including config_util for handling configuration files, pipeline_pb2 for protocol buffer message types, and Google's protobuf library for working with protocol buffers.\n",
    "\n",
    "2. The current pipeline configuration is read using the get_configs_from_pipeline_file function, which takes the path of pipeline.config file and returns a dictionary of configurations.\n",
    "\n",
    "3. A new TrainEvalPipelineConfig object is created. This object will be populated with the configurations from the pipeline.config file.\n",
    "\n",
    "4. The pipeline.config file is opened and its contents are read into a string.\n",
    "\n",
    "5. The configuration string is merged into the TrainEvalPipelineConfig object using the Merge function from text_format.\n",
    "\n",
    "6. The test record path in the configuration object is updated with the actual path of your test record file.\n",
    "\n",
    "7. The updated configuration object is converted back to a string using MessageToString.\n",
    "\n",
    "8. The updated configuration string is written back to the pipeline.config file.\n",
    "\n",
    "The updated pipeline.config file now contains the correct paths for the test record file, and can be used to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9hRrO_ppfDF"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "# Load the pipeline.config file into a config object\n",
    "config = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "\n",
    "# Create a new TrainEvalPipelineConfig object\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "\n",
    "# Read the pipeline.config file and merge it into the TrainEvalPipelineConfig object\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)\n",
    "\n",
    "# Update the test record file path in the config object\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths['ANNOTATION_PATH'], 'test.record')]\n",
    "\n",
    "# Convert the updated config object back into a string\n",
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "\n",
    "# Write the updated config string back to the pipeline.config file\n",
    "with tf.io.gfile.GFile(files['PIPELINE_CONFIG'], \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr3ON7xMpfDG"
   },
   "source": [
    "# 6. Train the model\n",
    "In this section of the code, we are setting up and running the model training process:\n",
    "\n",
    "1. The script to train the model, model_main_tf2.py, is located within the TensorFlow's object_detection module. A path to this script is constructed.\n",
    "\n",
    "2. A command is constructed to run the training script using Python. This command includes the following options:\n",
    "\n",
    "- --model_dir: This is the path where the model checkpoints will be saved during training.\n",
    "- --pipeline_config_path: This is the path to the pipeline.config file, which was prepared and updated in the previous steps.\n",
    "- --num_train_steps: This sets the number of steps for which to train the model. In this case, it's set to 2000, but this can be increased if you have more data or want the model to learn for a longer period.\n",
    "3. The constructed command is printed, and then run using the ! operator, which is specific to Jupyter notebooks and is used to run shell commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the training script\n",
    "TRAINING_SCRIPT = os.path.join(paths['APIMODEL_PATH'], 'research', 'object_detection', 'model_main_tf2.py')\n",
    "\n",
    "# Create the command to run the training script\n",
    "command = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps=2000\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'])\n",
    "\n",
    "# Print the command\n",
    "print(command)\n",
    "\n",
    "# Execute the command\n",
    "!{command}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code sets up and executes the model training process. The training script, model_main_tf2.py, is located within the TensorFlow's object_detection module. A command is then constructed to run this script with specific options indicating where to save the model checkpoints (--model_dir), where to find the pipeline.config file (--pipeline_config_path), and how many steps to train the model for (--num_train_steps). This command is then executed in the shell using the ! operator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4_YRZu7npfDH"
   },
   "source": [
    "# 7. Evaluate the Model\n",
    "In this section of the code, we are preparing the command to run the model evaluation process:\n",
    "\n",
    "A command is constructed to run the same training script as before, model_main_tf2.py, but with different options. This command includes the following options:\n",
    "- --model_dir: This is the path where the model checkpoints were saved during training.\n",
    "- --pipeline_config_path: This is the path to the pipeline.config file.\n",
    "- --checkpoint_dir: This is the path where the model checkpoints are stored, and from where the model to be evaluated will be loaded. In this case, it's the same as model_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80L7-fdPpfDH"
   },
   "outputs": [],
   "source": [
    "command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths['CHECKPOINT_PATH'],files['PIPELINE_CONFIG'], paths['CHECKPOINT_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYsgEPx9pfDH",
    "outputId": "8632d48b-91d2-45d9-bcb8-c1b172bf6eed"
   },
   "outputs": [],
   "source": [
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqTV2jGBpfDH"
   },
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the code prepares the command to run the model evaluation process. The same training script, model_main_tf2.py, is used, but with different options. The --model_dir option specifies the path to the model checkpoints, the --pipeline_config_path option specifies the path to the pipeline.config file, and the --checkpoint_dir option specifies the path where the model checkpoints are stored and from where the model to be evaluated will be loaded. The constructed command is stored in the variable command for later execution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "orvRk02UpfDI"
   },
   "source": [
    "# 8. Load Train Model From Checkpoint\n",
    "In this section of the code, we are loading the trained model checkpoint and defining a function for object detection:\n",
    "\n",
    "1. The TensorFlow and object detection libraries are imported.\n",
    "2. The pipeline configuration is loaded using config_util.get_configs_from_pipeline_file, which retrieves the configurations from the pipeline.config file.\n",
    "3. The detection model is built using model_builder.build, with the model configuration obtained from the pipeline configurations.\n",
    "4. The model checkpoint is restored using tf.compat.v2.train.Checkpoint and the restore function. The checkpoint file is specified as the ckpt-11 file in the checkpoint directory.\n",
    "5. A detect_fn function is defined. This function takes an image as input, preprocesses it using the detection model's preprocess function, performs prediction using the predict function, and then postprocesses the predictions using the postprocess function. The resulting detections are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TYk4_oIpfDI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "from object_detection.utils import config_util\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(files['PIPELINE_CONFIG'])\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(paths['CHECKPOINT_PATH'], 'ckpt-11')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the code, we load the trained model checkpoint and define a function for object detection. First, the necessary libraries are imported. Then, the pipeline configuration is loaded using config_util.get_configs_from_pipeline_file. The detection model is built using model_builder.build with the model configuration from the pipeline. The model checkpoint is restored using tf.compat.v2.train.Checkpoint and restore. Finally, the detect_fn function is defined, which takes an image as input, preprocesses it using the detection model's preprocess function, performs prediction using the predict function, and postprocesses the predictions using the postprocess function. The resulting detections are returned."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0EmsmbBZpfDI"
   },
   "source": [
    "# 9. Detect from an Image\n",
    "In this final section of the code, we perform object detection on a test image using the loaded model:\n",
    "\n",
    "1. The OpenCV, NumPy, and Matplotlib libraries are imported.\n",
    "2. The category index is created using label_map_util.create_category_index_from_labelmap based on the label map file.\n",
    "3. The test image path is specified and read using cv2.imread.\n",
    "4. The image is converted to a NumPy array.\n",
    "5. The input tensor is created by converting the image array to a TensorFlow tensor.\n",
    "6. The detect_fn function is called with the input tensor to obtain the detections.\n",
    "7. The number of detections is extracted and the detections dictionary is updated accordingly.\n",
    "8. The detection classes are converted to integers and the label ID offset is applied.\n",
    "9. A copy of the image is created for visualization purposes.\n",
    "10. The visualize_boxes_and_labels_on_image_array function from viz_utils is called to draw bounding boxes and labels on the image.\n",
    "11. The resulting image with detections is displayed using plt.imshow and plt.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "Tpzn1SMry1yK",
    "outputId": "c392a2c5-10fe-4fc4-9998-a1d4c7db2bd3"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib tk\n",
    "\n",
    "# Create category index\n",
    "category_index = label_map_util.create_category_index_from_labelmap(files['LABELMAP'])\n",
    "\n",
    "# Load and preprocess the test image\n",
    "IMAGE_PATH = os.path.join(paths['IMAGE_PATH'], 'test', 'jorge6.jpg')\n",
    "img = cv2.imread(IMAGE_PATH)\n",
    "image_np = np.array(img)\n",
    "\n",
    "# Convert image to TensorFlow tensor\n",
    "input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "\n",
    "# Perform object detection\n",
    "detections = detect_fn(input_tensor)\n",
    "\n",
    "# Process detection results\n",
    "num_detections = int(detections.pop('num_detections'))\n",
    "detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "detections['num_detections'] = num_detections\n",
    "\n",
    "# Convert detection classes to integers and apply label ID offset\n",
    "label_id_offset = 1\n",
    "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "# Create a copy of the image for visualization\n",
    "image_np_with_detections = image_np.copy()\n",
    "\n",
    "# Draw bounding boxes and labels on the image\n",
    "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "    image_np_with_detections,\n",
    "    detections['detection_boxes'],\n",
    "    detections['detection_classes'] + label_id_offset,\n",
    "    detections['detection_scores'],\n",
    "    category_index,\n",
    "    use_normalized_coordinates=True,\n",
    "    max_boxes_to_draw=100,\n",
    "    min_score_thresh=0.26,\n",
    "    agnostic_mode=False\n",
    ")\n",
    "\n",
    "# Display the image with detections\n",
    "plt.imshow(cv2.cvtColor(image_np_with_detections, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section of the code, object detection is performed on a test image using the loaded model. The necessary libraries are imported, including OpenCV, NumPy, and Matplotlib. The category index is created from the label map file. The test image is loaded and preprocessed. The image is converted to a TensorFlow tensor and passed through the detect_fn function to obtain the detections. The detection results are processed and visualized by drawing bounding boxes and labels on a copy of the image. Finally, the image with detections is displayed using Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsNAaYAo0WVL"
   },
   "source": [
    "# 10. Real Time Detections from your Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall opencv-python-headless -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_grs6OGpfDJ"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    \n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections = detect_fn(input_tensor)\n",
    "    \n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=5,\n",
    "                min_score_thresh=.8,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "3. Training and Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
